---
title: "analysis"
format: html
editor: visual
---

# WARNING

WHEN RERUNNING THE CODE, TEST THAT ALL PEOPLE WHO GOT MARRIED AFTER 40 AREN'T ALL BEING COUNTED AS MARRYING EXACTLY AT 40.

## Known Issues

-   Spain, Polish, and 1997 US additional data sets in the original data set are exclusively women
-   2nd spanish dataset also has overwhelming majority of women

## Solved

Added a missing by design level to the Netherlands and Hungary

-   Hungary gets deleted because it's missing a nativity indicator - should add a category missing by design, so does a whole survey in the Netherlands

Just threw out the BHPS, too much of a headache

-   The UK has a bunch of 0 weight observations - these are extension statistics for (1500 Scotland and Wales, 2000 Northern Ireland) - these should be thrown out, they are for making separate analyses of those countries, the original sample is nationally representative
-   UK BHPS and German surveys have a lot of missing values on education
    -   UK BHPS had very sparse education records

    -   UK BHPS also has patchy start dates of first unions

    -   I think I will just get rid of them from the analysis, seeing as I've got GGS for them now

## Initial setup

```{r library}
# General functionality
library(tidyverse)
library(flextable)
library(officer)

# For importing the data
library(sjmisc)
library(haven)
library(janitor)
library(countrycode)

# model evaluation
library(broom)
library(marginaleffects)
library(sandwich)
library(lmtest)

# for KM curves
library(survival)
library(ggsurvfit)
library(pammtools)

# possible speeding up models
library(speedglm)

# meta-regression
library(metafor)

# MAAS from speedglm messes with this
select <- dplyr::select
```

```{r issue-button}
# countries with missing ISLEDs
analysis_select |> filter(is.na(isled)) |> group_by(country, round, isled) |> summarise(n = n_distinct(respid)) |> print(n=500)

# countries with missing foreign_born
analysis_select |> filter(is.na(foreign_born)) |> group_by(country, round, foreign_born) |> summarise(n = n_distinct(respid)) |> print(n=500)

```

## Preparation

Winsorisation is a necessary steps, there are obviously wrong weights in this dataset.

```{r weights}
# Define parameters
winsor_factor <- 3
max_iterations <- 15

# Start the pipe
analysis_weights <- analysis_select |> 
  # Tag one observation per respondent
  group_by(country, round, respid) |>
  mutate(oneobs = row_number() == 1) |>
  ungroup()

# Iterative winsorization and rescaling
for (i in 1:max_iterations) {
  analysis_weights <- analysis_weights |>
    group_by(country, round) |>
    mutate(
      median_weight = median(perswgt[oneobs], na.rm = TRUE),
      perswgt = if_else(
        oneobs & !is.na(perswgt) & perswgt > winsor_factor * median_weight,
        winsor_factor * median_weight,
        perswgt
      ),
      mean_weight = mean(perswgt[oneobs], na.rm = TRUE),
      perswgt = if_else(
        oneobs & !is.na(perswgt),
        perswgt / mean_weight,
        perswgt
      )
    ) |>
    select(-median_weight, -mean_weight) |>
    ungroup()
}

# Spread corrected weights to all person-observations
analysis_weights <- analysis_weights |>
  group_by(country, round, respid) |>
  arrange(desc(oneobs)) |>
  mutate(perswgt = first(perswgt)) |>  # Simply replace all with first
  ungroup() |>
  select(-oneobs) |> 
  arrange(country, round, respid) |> 
  mutate(naive_perswgt = if_else(is.na(perswgt), 1, perswgt))

# write.csv(analysis_weights, file = paste0(data_path, "/intermediate_output/analysis_weights.csv"))
```

## Creation of country-cohorts and analytical sample

```{r country-cohorts}
analysis_proper <- analysis_weights |>
  
  # add model covariates (and unknown marriage year)
  drop_na(isled, time, enrol_mod, foreign_born, male, in_union, event) |>
  
  # Creation of country-cohorts and standardisation of ISLED (across contexts)
  mutate(
  marriage_cohort_cat = cut(
    marriage_cohort,
    breaks = c(-Inf, seq(1951, 2011, by = 10), Inf),
    labels = c("Before 1951", "1951-1960", "1961-1970", 
               "1971-1980", "1981-1990", "1991-2000", 
               "2001-2010", "After 2010"),
    right = FALSE,
    include.lowest = TRUE
  ),
  
  # Manually combine small cohorts by country
  marriage_cohort_cat = case_when(
    # Austria: combine After 2010 with 2001-2010
    country == "Austria" & marriage_cohort_cat %in% c("2001-2010", "After 2010") ~ "After 2000",
    
    # Belarus: combine After 2010 with 2001-2010
    country == "Belarus" & marriage_cohort_cat %in% c("2001-2010", "After 2010") ~ "After 2000",
    country == "Kazakhstan" & marriage_cohort_cat %in% c("1961-1970", "1971-1980") ~ "Before 1981",
    
    # Belgium: combine 2001-2010 with After 2010, then with 1991-2000
    country == "Belgium" & marriage_cohort_cat %in% c("1951-1960", "1961-1970") ~ "Before 1971",
    country == "Belgium" & marriage_cohort_cat %in% c("1991-2000", "2001-2010", "After 2010") ~ "After 1990",
    
    # Bulgaria: combine 2001-2010 with After 2010, then with 1991-2000
    country == "Bulgaria" & marriage_cohort_cat %in% c("1991-2000", "2001-2010", "After 2010") ~ "After 1990",
    
    # Canada: perfect prediction after 2010
    country == "Canada" & marriage_cohort_cat %in% c("2001-2010", "After 2010") ~ "After 2001",
    
    # Croatia: combine all before 1991 into one group, and After 2000 with 1991-2000
    country == "Croatia" & marriage_cohort_cat %in% c("Before 1951", "1951-1960", "1961-1970", "1971-1980", "1981-1990") ~ "Before 1991",
    country == "Croatia" & marriage_cohort_cat %in% c("1991-2000", "2001-2010", "After 2010") ~ "After 1990",
    
    # Czechia: combine After 2010 with 2001-2010
    country == "Czechia" & marriage_cohort_cat %in% c("2001-2010", "After 2010", "After 2020") ~ "After 2000",
    
    # Denmark: combine After 2010 with 2001-2010
    country == "Denmark" & marriage_cohort_cat %in% c("2001-2010", "After 2010") ~ "After 2000",
    
    # Estonia: combine After 2010 with 2001-2010
    country == "Estonia" & marriage_cohort_cat %in% c("2001-2010", "After 2010") ~ "After 2000",
    
    # Finland: combine all before 1991 into one group, and After 2000 with 1991-2000
    country == "Finland" & marriage_cohort_cat %in% c("Before 1951", "1951-1960", "1961-1970", "1971-1980", "1981-1990") ~ "Before 1991",
    country == "Finland" & marriage_cohort_cat %in% c("1991-2000", "2001-2010", "After 2010") ~ "After 1990",
    
    # France: combine all 1991 onwards
    country == "France" & marriage_cohort_cat %in% c("1991-2000", "2001-2010", "After 2010") ~ "After 1990",
    
    # Georgia: combine After 2000 with 1991-2000
    country == "Georgia" & marriage_cohort_cat %in% c("1991-2000", "2001-2010", "After 2010") ~ "After 1990",
    country == "Georgia" & marriage_cohort_cat %in% c("1951-1960", "1961-1970") ~ "Before 1971",
    
    # Germany: combine After 2010 with 2001-2010
    country == "Germany" & marriage_cohort_cat %in% c("2001-2010", "After 2010") ~ "After 2000",
    
    # Italy: combine all from 1991 onwards
    country == "Italy" & marriage_cohort_cat %in% c("1991-2000", "2001-2010", "After 2010") ~ "After 1990",
    
    # Kazakhstan: combine After 2010 with 2001-2010
    country == "Kazakhstan" & marriage_cohort_cat %in% c("2001-2010", "After 2010") ~ "After 2000",
    country == "Kazakhstan" & marriage_cohort_cat %in% c("1961-1970", "1971-1980") ~ "Before 1981",
    
    # Lithuania: combine After 2000 with 1991-2000
    country == "Lithuania" & marriage_cohort_cat %in% c("1991-2000", "2001-2010", "After 2010") ~ "After 1990",
    
    # Moldova
    country == "Moldova" & marriage_cohort_cat %in% c("2001-2010", "After 2010") ~ "After 2000",
    
    # Netherlands:
    country == "Netherlands" & marriage_cohort_cat %in% c("1991-2000", "2001-2010") ~ "After 1990",
    
    # Norway: combine After 2010 with 2001-2010
    country == "Norway" & marriage_cohort_cat %in% c("2001-2010", "After 2010") ~ "After 2000",
    
    # Poland
    country == "Poland" & marriage_cohort_cat %in% c("1951-1960", "1961-1970") ~ "Before 1971",
    
    # Romania: combine all from 1991 onwards
    country == "Romania" & marriage_cohort_cat %in% c("1991-2000", "2001-2010", "After 2010") ~ "After 1990",
    
    # Russia: combine all from 1991 onwards
    country == "Russia" & marriage_cohort_cat %in% c("1991-2000", "2001-2010", "After 2010") ~ "After 1990",
    
    # Spain: combine After 2010 with 2001-2010
    country == "Spain" & marriage_cohort_cat %in% c("2001-2010", "After 2010") ~ "After 2000",
    country == "Spain" & marriage_cohort_cat %in% c("1951-1960", "1961-1970") ~ "Before 1971",
    
    # Sweden:
    country == "Sweden" & marriage_cohort_cat %in% c("1991-2000", "2001-2010") ~ "After 1990",
    
    # United Kingdom: combine After 2010 with 2001-2010
    country == "United Kingdom" & marriage_cohort_cat %in% c("2001-2010", "After 2010") ~ "After 2000",
    
    # United States:
    country == "United States" & marriage_cohort_cat %in% c("1991-2000", "2001-2010") ~ "After 1990",
    
    # Uruguay: combine After 2010 with 2001-2010
    country == "Uruguay" & marriage_cohort_cat %in% c("2001-2010", "After 2010") ~ "After 2000",
    
    # Keep all others as-is
    TRUE ~ as.character(marriage_cohort_cat)
  ),
    
    country_cohort = paste0(country, "_", marriage_cohort_cat),
    
    # between country standardisation
    isled_standard = scale(isled)[,1]
  )
```

Country cohorts in need of being joined up

```{r}
analysis_proper |> group_by(country_cohort) |> count(event) |> filter(n<101)
```

## Add macrodata

-   Please run the macrodata file now.

## Connecting macro-indicators to country-cohorts

```{r scoring-country-cohorts}
macro_weighted <- analysis_proper |> 
  group_by(country, country_cohort, obs_year) |> 
  
  # number of Rs observed each year
  summarise(
    pweight = sum(naive_perswgt),
    .groups = "drop"
  ) |> 
  
  group_by(country_cohort) |> 
  
  # total years observed within each country-cohort
  mutate(
    totpweight = sum(pweight),
    oweight = pweight/totpweight, # percentage of union_years observed in each period
    cumweight = cumsum(oweight) # cumulative share of union_years observed
  ) |> 
  ungroup() |>
  dplyr::select(-pweight, -totpweight) |>
   
  
  # add the interpolated macrodata (onto each year)
  left_join(macro_indicators_interpolated |> rename(obs_year = time), by = c("country", "obs_year")) |> 
  
  group_by(country_cohort) |>
  
  # share of union years covered by each time series (e. g. 65% of union years experienced when the time series ends, and 35% of union were experienced by the time it began so in total the time series covers 30% of all union years experienced in a country-cohort - that being it's weight in the calculation then)
  mutate(
    across(
      .cols = where(is.numeric) & !any_of(c("obs_year", "oweight", "cumweight")),
      .fns = ~if(all(is.na(.))) 0 else 
              max(cumweight[!is.na(.)], na.rm = TRUE) - 
              min(cumweight[!is.na(.)], na.rm = TRUE),
      .names = "cov_{.col}"
    )
  ) |>
  ungroup() |>
  
  # aggregating the macro-level indicators for each country-cohort, weighted mean (years experienced by more respondents count for more)
  group_by(country, country_cohort) |>
  summarise(
    across(
      where(is.numeric) & !c(oweight, cumweight),
      ~if(startsWith(cur_column(), "cov_")) first(.) else weighted.mean(., oweight, na.rm = TRUE)
    ),
    .groups = "drop"
  )
```

## Descriptives and supplementary information

#### Table of descriptives

```{r descriptives-table}
# Create table
fb_props <- prop.table(table(analysis_proper$foreign_born, useNA = "no"))

desc_table <- tibble(
  Variable = c(
    "Educational attainment (standardized)",
    "Time",
    "Time squared",
    "Male",
    "In union",
    "Enrolled",
    "Missing by design",
    "Not born outside country",
    "Born outside country"
  ),
  Mean = c(
    mean(analysis_proper$isled_standard, na.rm = TRUE),
    mean(analysis_proper$time, na.rm = TRUE),
    mean(analysis_proper$time^2, na.rm = TRUE),
    mean(analysis_proper$male, na.rm = TRUE),
    mean(analysis_proper$in_union, na.rm = TRUE),
    mean(analysis_proper$enrol_mod, na.rm = TRUE),
    fb_props
  ),
  SD = c(
    sd(analysis_proper$isled_standard, na.rm = TRUE),
    sd(analysis_proper$time, na.rm = TRUE),
    sd(analysis_proper$time^2, na.rm = TRUE),
    NA, NA, NA, NA, NA, NA
  ),
  Min = c(
    min(analysis_proper$isled_standard, na.rm = TRUE),
    min(analysis_proper$time, na.rm = TRUE),
    min(analysis_proper$time^2, na.rm = TRUE),
    min(analysis_proper$male, na.rm = TRUE),
    min(analysis_proper$in_union, na.rm = TRUE),
    min(analysis_proper$enrol_mod, na.rm = TRUE),
    NA, NA, NA
  ),
  Max = c(
    max(analysis_proper$isled_standard, na.rm = TRUE),
    max(analysis_proper$time, na.rm = TRUE),
    max(analysis_proper$time^2, na.rm = TRUE),
    max(analysis_proper$male, na.rm = TRUE),
    max(analysis_proper$in_union, na.rm = TRUE),
    max(analysis_proper$enrol_mod, na.rm = TRUE),
    NA, NA, NA
  )
)

# Add sample size rows
sample_info <- tibble(
  Variable = c("N respondents", "N observations", "N events"),
  Mean = c(
    n_distinct(analysis_proper$respid),
    nrow(analysis_proper),
    sum(analysis_proper$event, na.rm = TRUE)
  ),
  SD = c(NA, NA, NA),
  Min = c(NA, NA, NA),
  Max = c(NA, NA, NA)
)

desc_table <- bind_rows(desc_table, sample_info)

# Format numbers
desc_table <- desc_table |>
  mutate(
    Mean = format(round(Mean, 2), nsmall = 2, big.mark = ","),
    SD = ifelse(is.na(SD), "", format(round(SD, 2), nsmall = 2)),
    Min = format(round(Min, 2), nsmall = 2),
    Max = format(round(Max, 2), nsmall = 2)
  ) |> 
  add_row(
    Variable = "Migration status",
    Mean = NA,
    SD = NA,
    Min = NA,
    Max = NA,
    .before = 7  # Adjust this number to where you want it inserted
  )

# Create flextable
ft <- flextable(desc_table) |>
  set_header_labels(
    Variable = "",
    Mean = "Mean",
    SD = "SD",
    Min = "Min",
    Max = "Max"
  ) |>
  align(j = 2:5, align = "right", part = "all") |>
  align(j = 1, align = "left", part = "all") |>
  font(fontname = "Times New Roman", part = "all") |>
  fontsize(size = 11, part = "all") |>
  bold(part = "header") |>
  hline(i = 10) |>
  italic(i = 12:13, j = 1) |>
  padding(i = 8:10, j = 1, padding.left = 20) |>  # Indent the three migration rows
  autofit()

# Export to Word
save_as_docx(ft, path = paste0(output_tables, "/descriptive_statistics_table.docx"))
```

#### Kaplan-Meier Curve

I always just need to keep the last row, the outcome variable will be event.

```{r KM-curves}
km_plotting_data <- analysis_proper |>
  select(respid, time, event, isled, naive_perswgt) |> 
  group_by(respid) |> 
  filter(time == max(time)) |> 
  ungroup() |> 
  mutate(
    isled_cat = case_when(
      isled > mean(isled, na.rm = TRUE) + 0.5 * sd(isled, na.rm = TRUE) ~ "High (>+0.5 SD)",
      isled < mean(isled, na.rm = TRUE) - 0.5 * sd(isled, na.rm = TRUE) ~ "Low (<-0.5 SD)",
      TRUE ~ "Medium (-0.5 to +0.5 SD)"
    ),
    isled_cat = factor(isled_cat, 
                       levels = c("High (>+0.5 SD)", "Medium (-0.5 to +0.5 SD)", "Low (<-0.5 SD)"),
                       ordered = TRUE)
  ) |> 
  select(-isled)

km_plot <- survfit2(Surv(time, event) ~ isled_cat, km_plotting_data, weights = naive_perswgt) |> 
  tidy() |> 
  separate(strata, 
           into = c("measure", "value"), 
           sep = "=", 
           remove = TRUE) |>
  mutate(
    estimate = 1 - estimate,
    conf.high = 1 - conf.high,
    conf.low = 1 - conf.low,
    time = time + 14  # Add 14 to shift 1->15, 15->40
  ) |> 
  bind_rows(
    tibble(
      value = c("High (>+0.5 SD)", "Medium (-0.5 to +0.5 SD)", "Low (<-0.5 SD)"),
      time = 14,
      estimate = 0,
      conf.low = 0,
      conf.high = 0
    )
  ) |> 
  ggplot(aes(x = time, y = estimate, group = value, ymin = conf.low, ymax = conf.high)) +
    geom_stepribbon(aes(color = value, fill = value)) +
    geom_step(color = "grey") +
    scale_y_continuous(labels = scales::percent) +
    scale_fill_discrete(breaks = c("High (>+0.5 SD)", "Medium (-0.5 to +0.5 SD)", "Low (<-0.5 SD)")) +
    scale_color_discrete(breaks = c("High (>+0.5 SD)", "Medium (-0.5 to +0.5 SD)", "Low (<-0.5 SD)")) +
    labs(
      color = "Educational attainment",
      fill = "Educational attainment",
      y = "Married (%)",
      x = "Age"
    ) + 
    theme_bw()

ggsave(
  filename = paste0(output_plots, "/kaplan_meier.png"),
  plot = km_plot,
  width = 10,
  height = 5.625,
  dpi = 300
)
```

#### Supplementary info

Table with distribution of marriages by context

```{r events by context}
table_context <- analysis_proper |> 
  group_by(respid) |> 
  filter(time == max(time)) |> 
  ungroup() |> 
  
  count(country, marriage_cohort_cat, event) |> 
  mutate(
    Context = paste(country, marriage_cohort_cat),
    event = case_when(
      event == 0 ~ "Not married",
      event == 1 ~ "Married",
      TRUE ~ NA
    )
  ) |> 
  select(-country, -marriage_cohort_cat) |> 
  pivot_wider(id_cols = Context, names_from = event, values_from = n) |> 
  mutate(
    Total = `Not married` + Married,
    Prop_married = Married/Total
  )
  
table_context <- table_context |> 
  bind_rows(
    table_context |> 
      summarise(
        Context = "Total",
        `Not married` = sum(`Not married`),
        Married = sum(Married),
        Total = sum(Total)
      )
  )

# Create flextable
ft <- flextable(table_context) |> 
  set_header_labels(
    Context = "Context",
    `Not married` = "Not married",
    Married = "Married",
    Total = "Total",
    Prop_married = "Proportion married"
  ) |> 
  align(j = 2:4, align = "right", part = "all") |> 
  align(j = 1, align = "left", part = "all") |> 
  font(fontname = "Times New Roman", part = "all") |> 
  fontsize(size = 10, part = "all") |> 
  bold(part = "header") |> 
  bold(i = nrow(table_context), part = "body") |>  # Bold the Total row
  hline(i = nrow(table_context) - 1) |>  # Line above Total row
  autofit()

save_as_docx(ft, path = paste0(output_tables, "/supplementary_context.docx"))
```

## Modelling micro-level

```{r models-micro}
# Model 1: Education + intercept
m1_speed <- speedglm(event ~ isled_standard,
          data = analysis_proper,
          family = binomial(link = "logit"),
          weights = naive_perswgt)

# Model 2: Education + intercept by context (country-cohort)
m2_speed <- speedglm(event ~ isled_standard + country_cohort - 1,
          data = analysis_proper,
          family = binomial(link = "logit"),
          weights = naive_perswgt)

# Model 3: Education + intercept by context + controls
m3_speed <- speedglm(event ~ isled_standard + time + I(time^2) + male + in_union + enrol_mod + foreign_born + country_cohort - 1,
          data = analysis_proper,
          family = binomial(link = "logit"),
          weights = naive_perswgt)

# Model 4: Education × context + intercept by context + controls
m4_speed <- speedglm(event ~ isled_standard:country_cohort + time + I(time^2) + male + in_union +  enrol_mod + foreign_born + country_cohort - 1,
          data = analysis_proper,
          family = binomial(link = "logit"),
          weights = naive_perswgt)
```

## AMEs

```{r speed-AME}
# Calculating AMEs with just 20 percent of the sample (otherwise it takes ages)
set.seed(48103)

analysis_sample <- analysis_proper |> 
  group_by(country_cohort) |> 
  slice_sample(prop = 0.2, weight_by = naive_perswgt, replace = FALSE) |> 
  ungroup()

ames_provisional <- 
  slopes(
    m4_speed,
    variables = "isled_standard",
    by = "country_cohort",
    newdata = analysis_sample
  ) |> 
  tidy() |> 
  rename(ame = estimate, se_ame = std.error) |> 
  
  # add baseline marriage rates
  left_join(
    analysis_proper |> 
      group_by(country_cohort) |> 
      summarise(baseline_mar = weighted.mean(event, naive_perswgt, na.rm = TRUE)),
    by = "country_cohort"
  ) |> 
  
  mutate(
    rescaled_ame = (100 * ame) / baseline_mar,
    rescaled_se = (100 * se_ame) / baseline_mar,
    rescaled_conf.low = (100 * conf.low) / baseline_mar,
    rescaled_conf.high = (100 * conf.high) / baseline_mar
  ) 

# write_csv(ames_provisional, file = paste0(data_path, "/intermediate_output/marginal_effects.csv"))

modelling <- ames_provisional |> 
  separate(country_cohort, 
           into = c("country", "cohort"), 
           sep = "_", 
           remove = FALSE) |> 
  select(country_cohort, country, cohort, starts_with("rescaled"), ame) |> 
  
  # add average marriage year by country-cohort
  left_join(
    analysis_proper|>
      group_by(respid) |>
      filter(row_number() == 1) |>
      ungroup() |> 
      group_by(country_cohort) |> 
      summarise(avg_birth_cohort = weighted.mean(year, naive_perswgt, na.rm = TRUE)),
    by = "country_cohort"
  )  |> 
  
  # add macro-covariates
  left_join(
    macro_weighted |> 
      select(
        country_cohort,
        gender_roles, cov_gender_roles,
        family_diversity_acceptance, cov_family_diversity_acceptance,
        std_educ_earnings_advantage, std_educ_unemployment_advantage, std_relative_lfp,
        cov_std_educ_unemployment_advantage, cov_std_educ_earnings_advantage, cov_std_relative_lfp),
    by = "country_cohort"
  )
  
 
```

## Margins plot

```{r margins-plot}
modelling <- modelling |> 
  mutate(region = countrycode(country, "country.name", "region23"))  |>  
  mutate(region = case_when(
    country == "United Kingdom" ~ "Western Europe",
    country == "United States" ~ "Western Europe",
    country == "Canada" ~ "Western Europe",
    country == "Estonia" ~ "Eastern Europe",
    country == "Lithuania" ~ "Eastern Europe",
    country == "Uruguay" ~ "Southern Europe",
    country == "Croatia" ~ "Eastern Europe",
    TRUE ~ region
  )) |> 
  mutate(region = case_when(
    region == "Eastern Europe" ~ "Central and Eastern Europe",
    region == "Western Europe" ~ "Western Europe and North America",
    region == "Western Asia" ~ "Central and Eastern Europe",
    region == "Central Asia" ~ "Central and Eastern Europe",
    region == "Southern Europe" ~ "Southern Europe and South America",
    TRUE ~ region
  ))

plots_by_region <- modelling |> 
  group_split(region) |>
  map(~ {
    region_name <- unique(.x$region)
    
    ggplot(.x, aes(x = avg_birth_cohort, y = rescaled_ame)) +
      geom_line() +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "solid", color = "grey50", linewidth = 1, alpha = 0.2) +
      scale_x_continuous(breaks = seq(1960, 2020, 10)) +
      facet_wrap(vars(country)) +
      labs(
        title = region_name,
        y = "Effect of education on marriage (pp)",
        x = "Birth cohort (avg. year of 15th birthday in cohort)"
      ) +
      theme_bw() +
      theme(panel.grid = element_blank())
  })

# Assign names based on the region (if not already named)
if (is.null(names(plots_by_region))) {
  names(plots_by_region) <- map_chr(plots_by_region, ~ unique(.x$data$region))
}

# Save each plot in the list
# Alternative using imap
imap(plots_by_region, ~ ggsave(
  filename = paste0(output_plots, "/", gsub(" ", "_", .y), ".png"),
  plot = .x,
  width = 10,
  height = 5.625,
  dpi = 300
))
```

## Meta-regression

## Obtaining weights

```{r meta-weights}
macrovars <- c("std_relative_lfp", "std_educ_unemployment_advantage", "std_educ_earnings_advantage", "family_diversity_acceptance", "gender_roles")

# Initialize weight columns
for (x in macrovars) {
  modelling[[paste0("w_", x)]] <- NA_real_
}

# Obtain meta-regression weights
cat("\n=== META-REGRESSION WEIGHTS ESTIMATION ===\n\n")

for (x in macrovars) {
  
  cat("Variable:", x, "\n")
  cat(rep("-", 60), "\n", sep = "")
  
  # Create complete data (no NAs in key variables)
  complete_data <- modelling  |> 
    filter(!is.na(rescaled_ame) & !is.na(rescaled_se) & !is.na(.data[[x]]))
  
  n_excluded <- nrow(modelling) - nrow(complete_data)
  
  cat("Total observations:", nrow(modelling), "\n")
  cat("Used in model:", nrow(complete_data), "\n")
  cat("Excluded (NAs):", n_excluded, "\n\n")
  
  # Fit random-effects meta-regression with REML
  meta_model <- rma(yi = rescaled_ame, 
                    sei = rescaled_se, 
                    mods = ~ get(x),
                    data = complete_data,
                    method = "REML",
                    test = "knha")
  
  # Print model summary
  cat("Model results:\n")
  print(summary(meta_model))
  cat("\n")
  
  # Extract meta-regression weights
  weights_raw <- weights(meta_model)
  
  # Rescale to average = 1
  weights_rescaled <- weights_raw / mean(weights_raw)
  
  cat("Weight summary:\n")
  cat("  Mean:", round(mean(weights_rescaled), 4), "\n")
  cat("  Range: [", round(min(weights_rescaled), 4), ",", 
      round(max(weights_rescaled), 4), "]\n\n")
  
  # Assign weights back to complete cases only
  modelling[[paste0("w_", x)]][!is.na(modelling$rescaled_ame) & 
                                !is.na(modelling$rescaled_se) & 
                                !is.na(modelling[[x]])] <- weights_rescaled
  
  cat("\n")
}

cat("=== WEIGHT CALCULATION COMPLETE ===\n\n")

# Summary of weights
cat("Weight coverage summary:\n")
for (x in macrovars) {
  w_col <- paste0("w_", x)
  n_nonmissing <- sum(!is.na(modelling[[w_col]]))
  cat(sprintf("  %-40s: %3d/%3d obs (%.1f%%)\n", 
              w_col, n_nonmissing, nrow(modelling), 
              100 * n_nonmissing / nrow(modelling)))
}

for (x in macrovars) {
  
  # Get coverage variable
  cov_var <- paste0("cov_", x)
  
  # Rescale coverage to average = 1
  cov_rescaled <- modelling[[cov_var]] / mean(modelling[[cov_var]], na.rm = TRUE)
  
  # Combine meta-regression and coverage weights
  modelling[[paste0("w_final_", x)]] <- modelling[[paste0("w_", x)]] * cov_rescaled
}
```

#### Metaregression itself

```{r weighted-meta-regression}
m_relative_lfp <- lm(rescaled_ame ~ std_relative_lfp, weights = w_final_std_relative_lfp ,data = modelling)

# going up on this scale means disadvantage to education
m_unemployment_adv <- lm(rescaled_ame ~ std_educ_unemployment_advantage, weights = w_final_std_educ_unemployment_advantage ,data = modelling)

# going up on this scale means theres more advantage to education
m_earnings_adv <- lm(rescaled_ame ~ std_educ_earnings_advantage, weights = w_final_std_educ_earnings_advantage ,data = modelling)

# higher is more traditional
m_family_diversity_acceptance <- lm(rescaled_ame ~ family_diversity_acceptance, weights = w_final_family_diversity_acceptance,data = modelling)

# higher is more egalitarian
m_gender_roles <-  lm(rescaled_ame ~ gender_roles, weights = w_final_gender_roles,data = modelling)
```

#### Output table

```{r metareg-output}
# List of models with descriptive names
models <- list(
  "Relative labor force participation" = m_relative_lfp,
  "Unemployment advantage" = m_unemployment_adv,
  "Earnings advantage" = m_earnings_adv,
  "Family diversity acceptance" = m_family_diversity_acceptance,
  "Gender roles" = m_gender_roles
)

# Extract coefficients, R², and sample size
presentation_table <- map_df(names(models), function(name) {
  model <- models[[name]]
  coef_info <- tidy(model) %>% filter(term != "(Intercept)")
  r2 <- glance(model)$r.squared
  n_contexts <- nobs(model)
  
  # Format coefficient with significance stars and SE
  stars <- case_when(
    coef_info$p.value < 0.001 ~ "***",
    coef_info$p.value < 0.01 ~ "**",
    coef_info$p.value < 0.05 ~ "*",
    TRUE ~ ""
  )
  
  moderation_effect <- paste0(
    format(round(coef_info$estimate, 1), nsmall = 1),
    stars,
    " (", format(round(coef_info$std.error, 1), nsmall = 1), ")"
  )
  
  tibble(
    ` ` = name,
    `Moderation effect` = moderation_effect,
    `R²` = format(round(r2, 2), nsmall = 2),
    `N contexts` = n_contexts
  )
})

# Calculate total contexts (assuming this is your total sample)
total_contexts <- nrow(modelling)

# Add percentage column
presentation_table <- presentation_table %>%
  mutate(`% of total` = paste0(round((`N contexts` / total_contexts) * 100, 1), "%"))

# Create flextable
ft <- flextable(presentation_table) %>%
  set_header_labels(
    ` ` = "",
    `Moderation effect` = "Moderation effect",
    `R²` = "R²",
    `N contexts` = "N contexts",
    `% of total` = "% of total"
  ) %>%
  align(j = 2:5, align = "right", part = "all") %>%
  align(j = 1, align = "left", part = "all") %>%
  font(fontname = "Times New Roman", part = "all") %>%
  fontsize(size = 11, part = "all") %>%
  bold(part = "header") %>%
  add_footer_lines("Note: *** p<0.001, ** p<0.01, * p<0.05. Standard errors in parentheses.") %>%
  align(align = "left", part = "footer") %>%
  fontsize(size = 10, part = "footer") %>%
  autofit()

# Export to Word
save_as_docx(ft, path = paste0(output_tables, "/moderation_results_table.docx"))
```

#### Supplementary table for macrovariable scores by context

```{r macro-vars-by-context}

```

#### Supplementary table for model output

```{r macro-vars-by-context}

```
